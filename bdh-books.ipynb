{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30b4f1f",
   "metadata": {
    "papermill": {
     "duration": 0.003814,
     "end_time": "2026-01-10T08:00:22.264946",
     "exception": false,
     "start_time": "2026-01-10T08:00:22.261132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BDH Training on Kaggle - Monte Cristo Dataset\n",
    "\n",
    "This notebook trains the Baby Dragon Hatchling (BDH) model on two classic novels:\n",
    "- **The Count of Monte Cristo** by Alexandre Dumas (~2.7 MB)\n",
    "- **In Search of the Castaways** by Jules Verne (~845 KB)\n",
    "\n",
    "**Total training data: ~3.5 MB of text**\n",
    "\n",
    "## Setup Instructions for Kaggle:\n",
    "1. **Enable GPU**: Settings → Accelerator → GPU T4 x2\n",
    "2. **Internet**: Settings → Internet → ON\n",
    "3. Upload these files to Kaggle input:\n",
    "   - `The Count of Monte Cristo.txt`\n",
    "   - `In search of the castaways.txt`\n",
    "4. Run all cells in order\n",
    "\n",
    "Training takes ~30-40 minutes on Kaggle's free GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c25d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:00:22.271916Z",
     "iopub.status.busy": "2026-01-10T08:00:22.271629Z",
     "iopub.status.idle": "2026-01-10T08:00:22.455445Z",
     "shell.execute_reply": "2026-01-10T08:00:22.454724Z"
    },
    "papermill": {
     "duration": 0.189366,
     "end_time": "2026-01-10T08:00:22.457207",
     "exception": false,
     "start_time": "2026-01-10T08:00:22.267841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 10 08:00:22 2026       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   37C    P0             28W /  250W |       0MiB /  16384MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92286be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:00:22.464549Z",
     "iopub.status.busy": "2026-01-10T08:00:22.464307Z",
     "iopub.status.idle": "2026-01-10T08:00:26.138709Z",
     "shell.execute_reply": "2026-01-10T08:00:26.137818Z"
    },
    "papermill": {
     "duration": 3.680043,
     "end_time": "2026-01-10T08:00:26.140248",
     "exception": false,
     "start_time": "2026-01-10T08:00:22.460205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared!\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "Available memory: 15.89 GB\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared!\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a14ac",
   "metadata": {
    "papermill": {
     "duration": 0.002918,
     "end_time": "2026-01-10T08:00:26.146219",
     "exception": false,
     "start_time": "2026-01-10T08:00:26.143301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clone Original BDH Repository from Pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f52940ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:00:26.153017Z",
     "iopub.status.busy": "2026-01-10T08:00:26.152648Z",
     "iopub.status.idle": "2026-01-10T08:00:27.201659Z",
     "shell.execute_reply": "2026-01-10T08:00:27.200901Z"
    },
    "papermill": {
     "duration": 1.054267,
     "end_time": "2026-01-10T08:00:27.203284",
     "exception": false,
     "start_time": "2026-01-10T08:00:26.149017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bdh'...\r\n",
      "remote: Enumerating objects: 77, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (26/26), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\r\n",
      "remote: Total 77 (delta 21), reused 9 (delta 9), pack-reused 51 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (77/77), 998.95 KiB | 25.61 MiB/s, done.\r\n",
      "Resolving deltas: 100% (28/28), done.\r\n",
      "/kaggle/working/bdh\n",
      "total 48\r\n",
      "drwxr-xr-x 4 root root 4096 Jan 10 08:00 .\r\n",
      "drwxr-xr-x 3 root root 4096 Jan 10 08:00 ..\r\n",
      "-rw-r--r-- 1 root root 5051 Jan 10 08:00 bdh.py\r\n",
      "drwxr-xr-x 2 root root 4096 Jan 10 08:00 figs\r\n",
      "drwxr-xr-x 8 root root 4096 Jan 10 08:00 .git\r\n",
      "-rw-r--r-- 1 root root   10 Jan 10 08:00 .gitignore\r\n",
      "-rw-r--r-- 1 root root 1072 Jan 10 08:00 LICENSE.md\r\n",
      "-rw-r--r-- 1 root root 4706 Jan 10 08:00 README.md\r\n",
      "-rw-r--r-- 1 root root   21 Jan 10 08:00 requirements.txt\r\n",
      "-rw-r--r-- 1 root root 3670 Jan 10 08:00 train.py\r\n"
     ]
    }
   ],
   "source": [
    "# Clone the official Pathway BDH repository\n",
    "!rm -rf bdh\n",
    "!git clone https://github.com/pathwaycom/bdh.git\n",
    "%cd bdh\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5eb1a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:00:27.211187Z",
     "iopub.status.busy": "2026-01-10T08:00:27.210833Z",
     "iopub.status.idle": "2026-01-10T08:00:31.144047Z",
     "shell.execute_reply": "2026-01-10T08:00:31.143239Z"
    },
    "papermill": {
     "duration": 3.939607,
     "end_time": "2026-01-10T08:00:31.146308",
     "exception": false,
     "start_time": "2026-01-10T08:00:27.206701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy tqdm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3b3f9",
   "metadata": {
    "papermill": {
     "duration": 0.003212,
     "end_time": "2026-01-10T08:00:31.152803",
     "exception": false,
     "start_time": "2026-01-10T08:00:31.149591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Verify BDH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff35c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:00:31.160626Z",
     "iopub.status.busy": "2026-01-10T08:00:31.160059Z",
     "iopub.status.idle": "2026-01-10T08:00:31.405082Z",
     "shell.execute_reply": "2026-01-10T08:00:31.404211Z"
    },
    "papermill": {
     "duration": 0.250872,
     "end_time": "2026-01-10T08:00:31.406802",
     "exception": false,
     "start_time": "2026-01-10T08:00:31.155930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BDH Model Configuration:\n",
      "  Layers: 6\n",
      "  Embedding dimension: 256\n",
      "  Attention heads: 4\n",
      "  Dropout: 0.1\n",
      "  Vocabulary size: 256 (byte-level)\n",
      "\n",
      "Total parameters: 25,296,896 (~25.3M)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib.util\n",
    "import torch\n",
    "\n",
    "# Load bdh module\n",
    "spec = importlib.util.spec_from_file_location(\"bdh\", \"bdh.py\")\n",
    "bdh = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"bdh\"] = bdh\n",
    "spec.loader.exec_module(bdh)\n",
    "\n",
    "# Show model configuration\n",
    "config = bdh.BDHConfig()\n",
    "print(\"BDH Model Configuration:\")\n",
    "print(f\"  Layers: {config.n_layer}\")\n",
    "print(f\"  Embedding dimension: {config.n_embd}\")\n",
    "print(f\"  Attention heads: {config.n_head}\")\n",
    "print(f\"  Dropout: {config.dropout}\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size} (byte-level)\")\n",
    "\n",
    "# Create model and show parameter count\n",
    "model = bdh.BDH(config)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,} (~{total_params/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f880ed5d",
   "metadata": {
    "papermill": {
     "duration": 0.003974,
     "end_time": "2026-01-10T08:00:31.415018",
     "exception": false,
     "start_time": "2026-01-10T08:00:31.411044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Training Data (Monte Cristo Books)\n",
    "\n",
    "**IMPORTANT**: Upload these files to Kaggle as Input Files:\n",
    "1. Go to \"Add Input\" → \"Upload\" → Select both .txt files\n",
    "2. They will be available at `/kaggle/input/your-dataset-name/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e73354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:00:31.423128Z",
     "iopub.status.busy": "2026-01-10T08:00:31.422515Z",
     "iopub.status.idle": "2026-01-10T08:00:31.432729Z",
     "shell.execute_reply": "2026-01-10T08:00:31.432037Z"
    },
    "papermill": {
     "duration": 0.015739,
     "end_time": "2026-01-10T08:00:31.433992",
     "exception": false,
     "start_time": "2026-01-10T08:00:31.418253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available input datasets:\n",
      "  /kaggle/input/bdh-book\n",
      "    - In search of the castaways.txt (0.81 MB)\n",
      "    - The Count of Monte Cristo.txt (2.66 MB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the input directory (Kaggle automatically mounts uploaded datasets)\n",
    "input_dirs = list(Path('/kaggle/input').glob('*'))\n",
    "print(\"Available input datasets:\")\n",
    "for dir in input_dirs:\n",
    "    print(f\"  {dir}\")\n",
    "    for file in dir.glob('*.txt'):\n",
    "        file_size_mb = file.stat().st_size / (1024 * 1024)\n",
    "        print(f\"    - {file.name} ({file_size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "483e9b51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:00:31.441667Z",
     "iopub.status.busy": "2026-01-10T08:00:31.440984Z",
     "iopub.status.idle": "2026-01-10T08:00:31.517559Z",
     "shell.execute_reply": "2026-01-10T08:00:31.516795Z"
    },
    "papermill": {
     "duration": 0.081949,
     "end_time": "2026-01-10T08:00:31.519075",
     "exception": false,
     "start_time": "2026-01-10T08:00:31.437126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded The Count of Monte Cristo.txt: 2,646,614 characters\n",
      "✓ Loaded In search of the castaways.txt: 826,131 characters\n",
      "\n",
      "✓ Total training data: 3,472,749 characters\n",
      "✓ Saved to monte_cristo_combined.txt\n",
      "\n",
      "First 200 characters:\n",
      "﻿The Project Gutenberg eBook of The Count of Monte Cristo\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restric\n"
     ]
    }
   ],
   "source": [
    "# Load and combine both books\n",
    "# MODIFY THIS PATH to match your uploaded dataset name\n",
    "data_dir = Path('/kaggle/input/bdh-book')  # Change this!\n",
    "\n",
    "books = [\n",
    "    'The Count of Monte Cristo.txt',\n",
    "    'In search of the castaways.txt'\n",
    "]\n",
    "\n",
    "combined_text = \"\"\n",
    "for book in books:\n",
    "    book_path = data_dir / book\n",
    "    if book_path.exists():\n",
    "        with open(book_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            text = f.read()\n",
    "            combined_text += text + \"\\n\\n\"  # Add separator\n",
    "        print(f\"✓ Loaded {book}: {len(text):,} characters\")\n",
    "    else:\n",
    "        print(f\"❌ File not found: {book_path}\")\n",
    "        print(f\"   Please upload it to Kaggle input!\")\n",
    "\n",
    "# Save combined text\n",
    "with open('monte_cristo_combined.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(combined_text)\n",
    "\n",
    "print(f\"\\n✓ Total training data: {len(combined_text):,} characters\")\n",
    "print(f\"✓ Saved to monte_cristo_combined.txt\")\n",
    "print(f\"\\nFirst 200 characters:\")\n",
    "print(combined_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478070be",
   "metadata": {
    "papermill": {
     "duration": 0.00325,
     "end_time": "2026-01-10T08:00:31.525782",
     "exception": false,
     "start_time": "2026-01-10T08:00:31.522532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Training Script with Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665f3bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:00:31.533529Z",
     "iopub.status.busy": "2026-01-10T08:00:31.533297Z",
     "iopub.status.idle": "2026-01-10T08:00:31.539162Z",
     "shell.execute_reply": "2026-01-10T08:00:31.538494Z"
    },
    "papermill": {
     "duration": 0.011621,
     "end_time": "2026-01-10T08:00:31.540578",
     "exception": false,
     "start_time": "2026-01-10T08:00:31.528957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_monte_cristo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_monte_cristo.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import importlib.util\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load bdh module\n",
    "spec = importlib.util.spec_from_file_location(\"bdh\", \"bdh.py\")\n",
    "bdh = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"bdh\"] = bdh\n",
    "spec.loader.exec_module(bdh)\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "print(f\"Using device: {device} with dtype {dtype}\")\n",
    "\n",
    "# Load data\n",
    "with open('monte_cristo_combined.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(f\"Training data: {len(data):,} characters\")\n",
    "\n",
    "# Convert to bytes\n",
    "data_bytes = bytearray(data, \"utf-8\")\n",
    "data_tensor = torch.tensor(data_bytes, dtype=torch.long, device=device)\n",
    "print(f\"Token count: {len(data_tensor):,}\")\n",
    "\n",
    "# Training parameters (adjusted for larger dataset)\n",
    "max_iters = 7000  # More iterations for bigger dataset\n",
    "eval_interval = 100\n",
    "batch_size = 8  # Larger batch\n",
    "block_size = 512  # Longer context\n",
    "learning_rate = 3e-4\n",
    "\n",
    "print(f\"\\nTraining parameters:\")\n",
    "print(f\"  Iterations: {max_iters}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Block size: {block_size}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "\n",
    "# Create model\n",
    "config = bdh.BDHConfig()\n",
    "model = bdh.BDH(config).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel parameters: {total_params:,}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training function\n",
    "def get_batch():\n",
    "    ix = torch.randint(len(data_tensor) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_tensor[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_tensor[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "for iter in tqdm(range(max_iters), desc=\"Training\"):\n",
    "    xb, yb = get_batch()\n",
    "    \n",
    "    # Forward pass with mixed precision\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=dtype):\n",
    "        logits, loss = model(xb, yb)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if iter % eval_interval == 0:\n",
    "        print(f\"\\nStep: {iter}/{max_iters} loss {loss.item():.3f}\")\n",
    "        \n",
    "        # Save checkpoint if best loss\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "\n",
    "print(f\"\\nTraining complete! Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Best loss: {best_loss:.4f}\")\n",
    "\n",
    "# Generate sample\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING SAMPLE TEXT\")\n",
    "print(\"=\"*60)\n",
    "model.eval()\n",
    "context = torch.tensor(\n",
    "    bytearray(\"The Count of Monte Cristo\", \"utf-8\"), \n",
    "    dtype=torch.long, \n",
    "    device=device\n",
    ").unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(context, max_new_tokens=300, temperature=0.8, top_k=10)\n",
    "    sample_text = bytes(generated.to(torch.uint8).to(\"cpu\").squeeze(0)).decode(errors=\"backslashreplace\")\n",
    "    print(sample_text)\n",
    "\n",
    "# Save the model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': config,\n",
    "    'training_info': {\n",
    "        'final_loss': losses[-1],\n",
    "        'best_loss': best_loss,\n",
    "        'iterations': max_iters,\n",
    "        'dataset': 'monte_cristo_combined',\n",
    "        'dataset_size': len(data),\n",
    "        'total_params': total_params,\n",
    "        'batch_size': batch_size,\n",
    "        'block_size': block_size,\n",
    "    },\n",
    "    'losses': losses,\n",
    "}\n",
    "\n",
    "save_path = os.path.join(save_dir, \"bdh_monte_cristo.pt\")\n",
    "torch.save(checkpoint, save_path)\n",
    "\n",
    "file_size_mb = os.path.getsize(save_path) / (1024 * 1024)\n",
    "print(f\"✓ Model saved to: {save_path}\")\n",
    "print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "print(f\"  Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"  Best loss: {best_loss:.4f}\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "print(f\"  Training data: {len(data):,} characters\")\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56f5bf",
   "metadata": {
    "papermill": {
     "duration": 0.003845,
     "end_time": "2026-01-10T08:00:31.547700",
     "exception": false,
     "start_time": "2026-01-10T08:00:31.543855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Training (30-40 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6feefac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T08:00:31.555472Z",
     "iopub.status.busy": "2026-01-10T08:00:31.554875Z",
     "iopub.status.idle": "2026-01-10T10:50:55.011273Z",
     "shell.execute_reply": "2026-01-10T10:50:55.010415Z"
    },
    "papermill": {
     "duration": 10223.46218,
     "end_time": "2026-01-10T10:50:55.013081",
     "exception": false,
     "start_time": "2026-01-10T08:00:31.550901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda with dtype torch.bfloat16\r\n",
      "Training data: 3,472,749 characters\r\n",
      "Token count: 3,551,719\r\n",
      "\r\n",
      "Training parameters:\r\n",
      "  Iterations: 7000\r\n",
      "  Batch size: 8\r\n",
      "  Block size: 512\r\n",
      "  Learning rate: 0.0003\r\n",
      "\r\n",
      "Model parameters: 25,296,896\r\n",
      "\r\n",
      "============================================================\r\n",
      "STARTING TRAINING\r\n",
      "============================================================\r\n",
      "Training:   0%|                                        | 0/7000 [00:00<?, ?it/s]\r\n",
      "Step: 0/7000 loss 5.593\r\n",
      "Training:   1%|▍                           | 100/7000 [02:26<2:47:41,  1.46s/it]\r\n",
      "Step: 100/7000 loss 2.511\r\n",
      "Training:   3%|▊                           | 200/7000 [04:52<2:45:15,  1.46s/it]\r\n",
      "Step: 200/7000 loss 2.406\r\n",
      "Training:   4%|█▏                          | 300/7000 [07:17<2:42:50,  1.46s/it]\r\n",
      "Step: 300/7000 loss 1.888\r\n",
      "Training:   6%|█▌                          | 400/7000 [09:43<2:40:21,  1.46s/it]\r\n",
      "Step: 400/7000 loss 1.591\r\n",
      "Training:   7%|██                          | 500/7000 [12:09<2:37:55,  1.46s/it]\r\n",
      "Step: 500/7000 loss 1.353\r\n",
      "Training:   9%|██▍                         | 600/7000 [14:35<2:35:30,  1.46s/it]\r\n",
      "Step: 600/7000 loss 1.316\r\n",
      "Training:  10%|██▊                         | 700/7000 [17:01<2:33:05,  1.46s/it]\r\n",
      "Step: 700/7000 loss 1.168\r\n",
      "Training:  11%|███▏                        | 800/7000 [19:26<2:30:38,  1.46s/it]\r\n",
      "Step: 800/7000 loss 1.230\r\n",
      "Training:  13%|███▌                        | 900/7000 [21:52<2:28:14,  1.46s/it]\r\n",
      "Step: 900/7000 loss 1.236\r\n",
      "Training:  14%|███▊                       | 1000/7000 [24:18<2:25:47,  1.46s/it]\r\n",
      "Step: 1000/7000 loss 1.264\r\n",
      "Training:  16%|████▏                      | 1100/7000 [26:44<2:23:21,  1.46s/it]\r\n",
      "Step: 1100/7000 loss 1.113\r\n",
      "Training:  17%|████▋                      | 1200/7000 [29:10<2:20:56,  1.46s/it]\r\n",
      "Step: 1200/7000 loss 1.134\r\n",
      "Training:  19%|█████                      | 1300/7000 [31:35<2:18:30,  1.46s/it]\r\n",
      "Step: 1300/7000 loss 1.175\r\n",
      "Training:  20%|█████▍                     | 1400/7000 [34:01<2:16:05,  1.46s/it]\r\n",
      "Step: 1400/7000 loss 1.072\r\n",
      "Training:  21%|█████▊                     | 1500/7000 [36:27<2:13:39,  1.46s/it]\r\n",
      "Step: 1500/7000 loss 1.060\r\n",
      "Training:  23%|██████▏                    | 1600/7000 [38:53<2:11:13,  1.46s/it]\r\n",
      "Step: 1600/7000 loss 0.938\r\n",
      "Training:  24%|██████▌                    | 1700/7000 [41:18<2:08:47,  1.46s/it]\r\n",
      "Step: 1700/7000 loss 1.074\r\n",
      "Training:  26%|██████▉                    | 1800/7000 [43:44<2:06:23,  1.46s/it]\r\n",
      "Step: 1800/7000 loss 1.091\r\n",
      "Training:  27%|███████▎                   | 1900/7000 [46:10<2:03:56,  1.46s/it]\r\n",
      "Step: 1900/7000 loss 1.002\r\n",
      "Training:  29%|███████▋                   | 2000/7000 [48:36<2:01:29,  1.46s/it]\r\n",
      "Step: 2000/7000 loss 1.052\r\n",
      "Training:  30%|████████                   | 2100/7000 [51:02<1:59:03,  1.46s/it]\r\n",
      "Step: 2100/7000 loss 1.044\r\n",
      "Training:  31%|████████▍                  | 2200/7000 [53:27<1:56:37,  1.46s/it]\r\n",
      "Step: 2200/7000 loss 1.067\r\n",
      "Training:  33%|████████▊                  | 2300/7000 [55:53<1:54:11,  1.46s/it]\r\n",
      "Step: 2300/7000 loss 1.056\r\n",
      "Training:  34%|█████████▎                 | 2400/7000 [58:19<1:51:48,  1.46s/it]\r\n",
      "Step: 2400/7000 loss 1.012\r\n",
      "Training:  36%|████████▉                | 2500/7000 [1:00:45<1:49:21,  1.46s/it]\r\n",
      "Step: 2500/7000 loss 0.986\r\n",
      "Training:  37%|█████████▎               | 2600/7000 [1:03:11<1:46:55,  1.46s/it]\r\n",
      "Step: 2600/7000 loss 1.000\r\n",
      "Training:  39%|█████████▋               | 2700/7000 [1:05:36<1:44:29,  1.46s/it]\r\n",
      "Step: 2700/7000 loss 0.988\r\n",
      "Training:  40%|██████████               | 2800/7000 [1:08:02<1:42:04,  1.46s/it]\r\n",
      "Step: 2800/7000 loss 0.921\r\n",
      "Training:  41%|██████████▎              | 2900/7000 [1:10:28<1:39:37,  1.46s/it]\r\n",
      "Step: 2900/7000 loss 0.934\r\n",
      "Training:  43%|██████████▋              | 3000/7000 [1:12:54<1:37:10,  1.46s/it]\r\n",
      "Step: 3000/7000 loss 0.982\r\n",
      "Training:  44%|███████████              | 3100/7000 [1:15:20<1:34:46,  1.46s/it]\r\n",
      "Step: 3100/7000 loss 0.870\r\n",
      "Training:  46%|███████████▍             | 3200/7000 [1:17:45<1:32:20,  1.46s/it]\r\n",
      "Step: 3200/7000 loss 0.848\r\n",
      "Training:  47%|███████████▊             | 3300/7000 [1:20:11<1:29:53,  1.46s/it]\r\n",
      "Step: 3300/7000 loss 0.999\r\n",
      "Training:  49%|████████████▏            | 3400/7000 [1:22:37<1:27:27,  1.46s/it]\r\n",
      "Step: 3400/7000 loss 0.902\r\n",
      "Training:  50%|████████████▌            | 3500/7000 [1:25:03<1:25:03,  1.46s/it]\r\n",
      "Step: 3500/7000 loss 0.957\r\n",
      "Training:  51%|████████████▊            | 3600/7000 [1:27:29<1:22:36,  1.46s/it]\r\n",
      "Step: 3600/7000 loss 0.893\r\n",
      "Training:  53%|█████████████▏           | 3700/7000 [1:29:54<1:20:11,  1.46s/it]\r\n",
      "Step: 3700/7000 loss 0.920\r\n",
      "Training:  54%|█████████████▌           | 3800/7000 [1:32:20<1:17:46,  1.46s/it]\r\n",
      "Step: 3800/7000 loss 0.922\r\n",
      "Training:  56%|█████████████▉           | 3900/7000 [1:34:46<1:15:20,  1.46s/it]\r\n",
      "Step: 3900/7000 loss 0.880\r\n",
      "Training:  57%|██████████████▎          | 4000/7000 [1:37:12<1:12:53,  1.46s/it]\r\n",
      "Step: 4000/7000 loss 0.900\r\n",
      "Training:  59%|██████████████▋          | 4100/7000 [1:39:38<1:10:28,  1.46s/it]\r\n",
      "Step: 4100/7000 loss 0.892\r\n",
      "Training:  60%|███████████████          | 4200/7000 [1:42:03<1:08:03,  1.46s/it]\r\n",
      "Step: 4200/7000 loss 0.845\r\n",
      "Training:  61%|███████████████▎         | 4300/7000 [1:44:29<1:05:37,  1.46s/it]\r\n",
      "Step: 4300/7000 loss 0.864\r\n",
      "Training:  63%|███████████████▋         | 4400/7000 [1:46:55<1:03:11,  1.46s/it]\r\n",
      "Step: 4400/7000 loss 0.855\r\n",
      "Training:  64%|████████████████         | 4500/7000 [1:49:21<1:00:44,  1.46s/it]\r\n",
      "Step: 4500/7000 loss 0.929\r\n",
      "Training:  66%|█████████████████▋         | 4600/7000 [1:51:47<58:19,  1.46s/it]\r\n",
      "Step: 4600/7000 loss 0.810\r\n",
      "Training:  67%|██████████████████▏        | 4700/7000 [1:54:12<55:53,  1.46s/it]\r\n",
      "Step: 4700/7000 loss 0.788\r\n",
      "Training:  69%|██████████████████▌        | 4800/7000 [1:56:38<53:27,  1.46s/it]\r\n",
      "Step: 4800/7000 loss 0.846\r\n",
      "Training:  70%|██████████████████▉        | 4900/7000 [1:59:04<51:01,  1.46s/it]\r\n",
      "Step: 4900/7000 loss 0.860\r\n",
      "Training:  71%|███████████████████▎       | 5000/7000 [2:01:30<48:36,  1.46s/it]\r\n",
      "Step: 5000/7000 loss 0.832\r\n",
      "Training:  73%|███████████████████▋       | 5100/7000 [2:03:56<46:10,  1.46s/it]\r\n",
      "Step: 5100/7000 loss 0.851\r\n",
      "Training:  74%|████████████████████       | 5200/7000 [2:06:21<43:44,  1.46s/it]\r\n",
      "Step: 5200/7000 loss 0.855\r\n",
      "Training:  76%|████████████████████▍      | 5300/7000 [2:08:47<41:18,  1.46s/it]\r\n",
      "Step: 5300/7000 loss 0.828\r\n",
      "Training:  77%|████████████████████▊      | 5400/7000 [2:11:13<38:52,  1.46s/it]\r\n",
      "Step: 5400/7000 loss 0.842\r\n",
      "Training:  79%|█████████████████████▏     | 5500/7000 [2:13:39<36:27,  1.46s/it]\r\n",
      "Step: 5500/7000 loss 0.813\r\n",
      "Training:  80%|█████████████████████▌     | 5600/7000 [2:16:05<34:00,  1.46s/it]\r\n",
      "Step: 5600/7000 loss 0.773\r\n",
      "Training:  81%|█████████████████████▉     | 5700/7000 [2:18:30<31:35,  1.46s/it]\r\n",
      "Step: 5700/7000 loss 0.842\r\n",
      "Training:  83%|██████████████████████▎    | 5800/7000 [2:20:56<29:09,  1.46s/it]\r\n",
      "Step: 5800/7000 loss 0.793\r\n",
      "Training:  84%|██████████████████████▊    | 5900/7000 [2:23:22<26:43,  1.46s/it]\r\n",
      "Step: 5900/7000 loss 0.912\r\n",
      "Training:  86%|███████████████████████▏   | 6000/7000 [2:25:48<24:18,  1.46s/it]\r\n",
      "Step: 6000/7000 loss 0.666\r\n",
      "Training:  87%|███████████████████████▌   | 6100/7000 [2:28:14<21:52,  1.46s/it]\r\n",
      "Step: 6100/7000 loss 0.779\r\n",
      "Training:  89%|███████████████████████▉   | 6200/7000 [2:30:39<19:26,  1.46s/it]\r\n",
      "Step: 6200/7000 loss 0.784\r\n",
      "Training:  90%|████████████████████████▎  | 6300/7000 [2:33:05<17:00,  1.46s/it]\r\n",
      "Step: 6300/7000 loss 0.826\r\n",
      "Training:  91%|████████████████████████▋  | 6400/7000 [2:35:31<14:34,  1.46s/it]\r\n",
      "Step: 6400/7000 loss 0.720\r\n",
      "Training:  93%|█████████████████████████  | 6500/7000 [2:37:57<12:08,  1.46s/it]\r\n",
      "Step: 6500/7000 loss 0.793\r\n",
      "Training:  94%|█████████████████████████▍ | 6600/7000 [2:40:23<09:43,  1.46s/it]\r\n",
      "Step: 6600/7000 loss 0.763\r\n",
      "Training:  96%|█████████████████████████▊ | 6700/7000 [2:42:48<07:17,  1.46s/it]\r\n",
      "Step: 6700/7000 loss 0.699\r\n",
      "Training:  97%|██████████████████████████▏| 6800/7000 [2:45:14<04:51,  1.46s/it]\r\n",
      "Step: 6800/7000 loss 0.710\r\n",
      "Training:  99%|██████████████████████████▌| 6900/7000 [2:47:40<02:25,  1.46s/it]\r\n",
      "Step: 6900/7000 loss 0.706\r\n",
      "Training: 100%|███████████████████████████| 7000/7000 [2:50:06<00:00,  1.46s/it]\r\n",
      "\r\n",
      "Training complete! Final loss: 0.7665\r\n",
      "Best loss: 0.6660\r\n",
      "\r\n",
      "============================================================\r\n",
      "GENERATING SAMPLE TEXT\r\n",
      "============================================================\r\n",
      "The Count of Monte Cristo, whose companion in the\r\n",
      "Château d’If, had returned from the company of his anger. Vampa raised\r\n",
      "his head, out of the table, closed the door open, and the door of\r\n",
      "Madame de Villefort ran there, and went to the kitchen. They were not\r\n",
      "the misfortune to do so, for he felt that she was agitating the\r\n",
      "e\r\n",
      "\r\n",
      "============================================================\r\n",
      "SAVING MODEL\r\n",
      "============================================================\r\n",
      "✓ Model saved to: saved_models/bdh_monte_cristo.pt\r\n",
      "  File size: 289.60 MB\r\n",
      "  Final loss: 0.7665\r\n",
      "  Best loss: 0.6660\r\n",
      "  Parameters: 25,296,896\r\n",
      "  Training data: 3,472,749 characters\r\n",
      "\r\n",
      "✓ Training complete!\r\n"
     ]
    }
   ],
   "source": [
    "# Run the training script\n",
    "!python train_monte_cristo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c5fa4",
   "metadata": {
    "papermill": {
     "duration": 0.254128,
     "end_time": "2026-01-10T10:50:55.517440",
     "exception": false,
     "start_time": "2026-01-10T10:50:55.263312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Verify Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a484e2aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:50:56.193609Z",
     "iopub.status.busy": "2026-01-10T10:50:56.192747Z",
     "iopub.status.idle": "2026-01-10T10:50:56.552727Z",
     "shell.execute_reply": "2026-01-10T10:50:56.551954Z"
    },
    "papermill": {
     "duration": 0.745659,
     "end_time": "2026-01-10T10:50:56.554253",
     "exception": false,
     "start_time": "2026-01-10T10:50:55.808594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully!\n",
      "\n",
      "Training Info:\n",
      "  final_loss: 0.7665\n",
      "  best_loss: 0.6660\n",
      "  iterations: 7000\n",
      "  dataset: monte_cristo_combined\n",
      "  dataset_size: 3472749\n",
      "  total_params: 25296896\n",
      "  batch_size: 8\n",
      "  block_size: 512\n",
      "\n",
      "Model file size: 289.60 MB\n",
      "Loss history points: 7000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check saved model\n",
    "model_path = \"saved_models/bdh_monte_cristo.pt\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    checkpoint = torch.load(model_path, weights_only=False)  # ← Added this parameter\n",
    "    \n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "    print(f\"\\nTraining Info:\")\n",
    "    for key, value in checkpoint['training_info'].items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nModel file size: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"Loss history points: {len(checkpoint['losses'])}\")\n",
    "else:\n",
    "    print(\"❌ Model not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86af90",
   "metadata": {
    "papermill": {
     "duration": 0.252327,
     "end_time": "2026-01-10T10:50:57.056356",
     "exception": false,
     "start_time": "2026-01-10T10:50:56.804029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load and Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2512f08f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:50:57.560342Z",
     "iopub.status.busy": "2026-01-10T10:50:57.560008Z",
     "iopub.status.idle": "2026-01-10T10:51:11.170845Z",
     "shell.execute_reply": "2026-01-10T10:51:11.170153Z"
    },
    "papermill": {
     "duration": 13.867877,
     "end_time": "2026-01-10T10:51:11.172577",
     "exception": false,
     "start_time": "2026-01-10T10:50:57.304700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded and ready for inference!\n",
      "\n",
      "============================================================\n",
      "GENERATED TEXT SAMPLES\n",
      "============================================================\n",
      "\n",
      "Prompt: 'Edmond Dantès'\n",
      "----------------------------------------\n",
      "Edmond Dantès,\n",
      "after thought that the tribe temperor returned to the powder and belong\n",
      "to him were a lesson than he had given them good, and the house was\n",
      "married by his master’s daughter. But that she was decei\n",
      "\n",
      "\n",
      "Prompt: 'The treasure of Monte Cristo'\n",
      "----------------------------------------\n",
      "The treasure of Monte Cristo was, was also with him\n",
      "that Marseilles support the coffer of the Count of Monte Cristo and to\n",
      "the hearth almost three hundred persons who were passing twenty years of\n",
      "excellent person, whose proceedi\n",
      "\n",
      "\n",
      "Prompt: 'Chapter 1.'\n",
      "----------------------------------------\n",
      "Chapter 1. Haydée\n",
      "Chapter 89. The Villefort Family Louis XVIII. A Smugglery\n",
      "Chapter 4. The Baron Danglars\n",
      "Chapter 10. The Breakfast\n",
      "Chapter 3. The Corsican Ogre\n",
      "Chapter 19. The Rue\n",
      "Chapter 8. Father and Son\n",
      "Ch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load('saved_models/bdh_monte_cristo.pt',weights_only=False)\n",
    "\n",
    "# Reconstruct model\n",
    "\n",
    "loaded_model = bdh.BDH(checkpoint['config'])\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"✓ Model loaded and ready for inference!\")\n",
    "\n",
    "# Test with custom prompts\n",
    "prompts = [\n",
    "    \"Edmond Dantès\",\n",
    "    \"The treasure of Monte Cristo\",\n",
    "    \"Chapter 1.\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATED TEXT SAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for custom_prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{custom_prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    context = torch.tensor(\n",
    "        bytearray(custom_prompt, \"utf-8\"), \n",
    "        dtype=torch.long, \n",
    "        device=device\n",
    "    ).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = loaded_model.generate(\n",
    "            context, \n",
    "            max_new_tokens=200, \n",
    "            temperature=0.8, \n",
    "            top_k=10\n",
    "        )\n",
    "        result = bytes(generated.to(torch.uint8).to(\"cpu\").squeeze(0)).decode(\n",
    "            errors=\"backslashreplace\"\n",
    "        )\n",
    "    \n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac3a29",
   "metadata": {
    "papermill": {
     "duration": 0.344901,
     "end_time": "2026-01-10T10:51:11.771159",
     "exception": false,
     "start_time": "2026-01-10T10:51:11.426258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download Model for Local Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20122698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T10:51:12.284291Z",
     "iopub.status.busy": "2026-01-10T10:51:12.283951Z",
     "iopub.status.idle": "2026-01-10T10:51:12.447072Z",
     "shell.execute_reply": "2026-01-10T10:51:12.446271Z"
    },
    "papermill": {
     "duration": 0.422127,
     "end_time": "2026-01-10T10:51:12.448531",
     "exception": false,
     "start_time": "2026-01-10T10:51:12.026404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model copied to /kaggle/working/bdh_monte_cristo.pt\n",
      "  Size: 289.60 MB\n",
      "\n",
      "To download:\n",
      "1. Click 'Save Version' (top right)\n",
      "2. Select 'Save & Run All'\n",
      "3. After completion, go to 'Output' tab\n",
      "4. Download bdh_monte_cristo.pt\n",
      "\n",
      "✓ You can now use this model with the long_inference.py script!\n"
     ]
    }
   ],
   "source": [
    "# On Kaggle, output files are automatically saved in /kaggle/working/\n",
    "# The model will be available in your output after committing the notebook\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Copy to Kaggle output directory\n",
    "output_path = \"/kaggle/working/bdh_monte_cristo.pt\"\n",
    "shutil.copy(\"saved_models/bdh_monte_cristo.pt\", output_path)\n",
    "\n",
    "print(f\"✓ Model copied to {output_path}\")\n",
    "print(f\"  Size: {os.path.getsize(output_path) / (1024*1024):.2f} MB\")\n",
    "print(\"\\nTo download:\")\n",
    "print(\"1. Click 'Save Version' (top right)\")\n",
    "print(\"2. Select 'Save & Run All'\")\n",
    "print(\"3. After completion, go to 'Output' tab\")\n",
    "print(\"4. Download bdh_monte_cristo.pt\")\n",
    "print(\"\\n✓ You can now use this model with the long_inference.py script!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1613a",
   "metadata": {
    "papermill": {
     "duration": 0.251181,
     "end_time": "2026-01-10T10:51:12.950484",
     "exception": false,
     "start_time": "2026-01-10T10:51:12.699303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What Was Done:\n",
    "1. ✅ Cloned official Pathway BDH repository\n",
    "2. ✅ Loaded Monte Cristo books (~3.5 MB combined)\n",
    "3. ✅ Trained BDH model for 5000 iterations\n",
    "4. ✅ Saved trained model with full checkpoint\n",
    "5. ✅ Verified model can generate text in Monte Cristo style\n",
    "\n",
    "### Model File Contents:\n",
    "- `model_state_dict`: All trained weights\n",
    "- `optimizer_state_dict`: Optimizer state\n",
    "- `config`: Model architecture\n",
    "- `training_info`: Losses, dataset info, hyperparameters\n",
    "- `losses`: Full training loss history\n",
    "\n",
    "### Next Steps:\n",
    "1. Download `bdh_monte_cristo.pt` from Kaggle output\n",
    "2. Use with `long_inference.py` to process 100k+ token books\n",
    "3. The model now understands Monte Cristo writing style!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9229149,
     "sourceId": 14448606,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10254.208958,
   "end_time": "2026-01-10T10:51:14.019581",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-10T08:00:19.810623",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
